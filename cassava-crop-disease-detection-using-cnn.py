{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13277,"databundleVersionId":422313,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport math\nimport glob\n# from skimage.transform import resize   # for resizing images\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#reading image values in pixels width * hight * channels\next = ['jpg', 'jpeg']    # Add image formats here\ndata = []\nlabels = []\nfiles = []\nimdir = '../input/train/train/cbb/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ndata.extend([cv2.imread(file) for file in files])\nlabels.extend([\"cbb\" for file in files])\nfiles = []\nimdir = '../input/train/train/cbsd/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ndata = np.concatenate([data, [cv2.imread(file) for file in files]])\nlabels.extend([\"cbsd\" for file in files])\nfiles = []\nimdir = '../input/train/train/cgm/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ndata = np.concatenate([data, [cv2.imread(file) for file in files]])\nlabels.extend([\"cgm\" for file in files])\nfiles = []\nimdir = '../input/train/train/cmd/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ndata = np.concatenate([data, [cv2.imread(file) for file in files]])\nlabels.extend([\"cmd\" for file in files])\nfiles = []\nimdir = '../input/train/train/healthy/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ndata = np.concatenate([data, [cv2.imread(file) for file in files]])\nlabels.extend([\"healthy\" for file in files])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nsize = (300, 300)\ndata = np.array([cv2.resize(d, size, interpolation = cv2.INTER_AREA) for d in data])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# data = np.concatenate([data, [np.fliplr(data[i]) for i in range(len(data))]])\n# labels.extend([labels[i] for i in range(len(labels))])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nlabels = np.array(labels)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nlr = [np.fliplr(data[i]) for i in range(len(data))]\nlabels_lr = [labels[i] for i in range(len(labels))]\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndata = np.concatenate([data, lr])\nlabels = np.concatenate([labels, labels_lr])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# data = np.array(data)\nlabels = np.array(labels)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nlabels = pd.get_dummies(labels)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nX_train = data\nY_train = labels\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# #using only training data\n# X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=.05, random_state=42, stratify=labels)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#using real test data\ntestData = []\nfiles = []\nimdir = '../input/test/test/0/'\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\ntestData.extend([cv2.imread(file) for file in files])\nsize = (300, 300)\ntestData = [cv2.resize(d, size, interpolation = cv2.INTER_AREA) for d in testData]\nX_train = data\nY_train = labels\nX_test = np.array(testData)\ntestLabels = files\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nimport numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ninput_shape = ((300, 300, 3))\nX_input = Input(input_shape)\n    \n# Zero-Padding: pads the border of X_input with zeroes\nX = ZeroPadding2D((3, 3))(X_input)\n# CONV -> BN -> RELU Block applied to X\nX = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\nX = BatchNormalization(axis = 3, name = 'bn0')(X)\nX = Activation('relu')(X)\n# MAXPOOL\nX = AveragePooling2D((2, 2), name='avg_pool0')(X)\n# CONV -> BN -> RELU Block applied to X\nX = Conv2D(32, (5, 5), strides = (1, 1), name = 'conv1')(X)\nX = BatchNormalization(axis = 3, name = 'bn1')(X)\nX = Activation('relu')(X)\n# MAXPOOL\nX = AveragePooling2D((2, 2), name='avg_pool1')(X)\n# CONV -> BN -> RELU Block applied to X\nX = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv2')(X)\nX = BatchNormalization(axis = 3, name = 'bn2')(X)\nX = Activation('relu')(X)\n# MAXPOOL\nX = AveragePooling2D((2, 2), name='avg_pool2')(X)\n# FLATTEN X (means convert it to a vector) + FULLYCONNECTED\nX = Flatten()(X)\nX = Dense(1024, activation=\"relu\")(X)\nX = Dropout(0.5)(X)\nX = Dense(5, activation='softmax', name='fc')(X)\n# Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\nmodel = Model(inputs = X_input, outputs = X, name='satellite')\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nmodel.summary()\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nmodel.compile(optimizer=\"sgd\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nmodel.fit(x=X_train, y=Y_train, epochs=50, batch_size=32)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# preds = model.evaluate(x=X_test, y=Y_test)\n# print()\n# print (\"Loss = \" + str(preds[0]))\n# print (\"Test Accuracy = \" + str(preds[1]))\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\npred = model.predict(x=X_test)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\npred = [np.argmax(pred[i]) for i in range(len(pred))]\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfor i in range(len(pred)):\n    if pred[i] == 0:\n        pred[i] = 'cbb'\n    if pred[i] == 1:\n        pred[i] = 'cbsd'\n    if pred[i] == 2:\n        pred[i] = 'cgm'\n    if pred[i] == 3:\n        pred[i] = 'cmd'\n    if pred[i] == 4:\n        pred[i] = 'healthy'\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ntestLabels = [l[21:] for l in testLabels]\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nsubmission = pd.DataFrame(list(zip(pred, testLabels)), columns=[\"Category\", \"Id\"])\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nsubmission.to_csv('submission.csv', index=False)\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n# create a link to download the dataframe\ncreate_download_link(submission)","metadata":{},"execution_count":null,"outputs":[]}]}